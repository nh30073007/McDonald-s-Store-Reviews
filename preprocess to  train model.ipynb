{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a60ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reviewer_id  store_name              category  \\\n",
      "0            1  McDonald's  Fast food restaurant   \n",
      "1            2  McDonald's  Fast food restaurant   \n",
      "2            3  McDonald's  Fast food restaurant   \n",
      "3            4  McDonald's  Fast food restaurant   \n",
      "4            5  McDonald's  Fast food restaurant   \n",
      "\n",
      "                                       store_address  latitude   longitude  \\\n",
      "0  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "1  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "2  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "3  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "4  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "\n",
      "  rating_count   review_time  \\\n",
      "0        1,240  3 months ago   \n",
      "1        1,240    5 days ago   \n",
      "2        1,240    5 days ago   \n",
      "3        1,240   a month ago   \n",
      "4        1,240  2 months ago   \n",
      "\n",
      "                                              review   rating  \n",
      "0  look like someone spit food normal transaction...   1 star  \n",
      "1  mcdonalds far food atmosphere go staff make di...  4 stars  \n",
      "2  make mobile order get speaker checked line mov...   1 star  \n",
      "3  mc crispy chicken sandwich customer service qu...  5 stars  \n",
      "4  repeat order time drive thru still manage mess...   1 star  \n"
     ]
    }
   ],
   "source": [
    "#Preprocess Step...........\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# LET'S DOWNLOAD NLTK RESOURCES\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATAPATH\n",
    "data_path = r\"C:\\Users\\nh013\\Desktop\\MC''donald\\McDonald_s_Reviews.csv\"\n",
    "df = pd.read_csv(data_path, encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FUNCTION\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # TOKENIZE TEXT\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #REMOVE PUNCTUATION AND CONVERT TO LOWER CASE\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    #REMOVE STOP WORDS\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # LEMMATIZE TOKEN\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    \n",
    "    # JOIN TOKEN BACK INTO A SINGLE STRING\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def get_wordnet_pos(token):\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# PERFROM PREPROCESSING TO THE  'review' COLUMN\n",
    "df['review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1c130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774235fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reviewer_id  store_name              category  \\\n",
      "0            1  McDonald's  Fast food restaurant   \n",
      "1            2  McDonald's  Fast food restaurant   \n",
      "2            3  McDonald's  Fast food restaurant   \n",
      "3            4  McDonald's  Fast food restaurant   \n",
      "4            5  McDonald's  Fast food restaurant   \n",
      "\n",
      "                                       store_address  latitude   longitude  \\\n",
      "0  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "1  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "2  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "3  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "4  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "\n",
      "  rating_count   review_time  \\\n",
      "0        1,240  3 months ago   \n",
      "1        1,240    5 days ago   \n",
      "2        1,240    5 days ago   \n",
      "3        1,240   a month ago   \n",
      "4        1,240  2 months ago   \n",
      "\n",
      "                                              review   rating  sentiment  \\\n",
      "0  look like someone spit food normal transaction...   1 star   0.216667   \n",
      "1  mcdonalds far food atmosphere go staff make di...  4 stars   0.368056   \n",
      "2  make mobile order get speaker checked line mov...   1 star  -0.100000   \n",
      "3  mc crispy chicken sandwich customer service qu...  5 stars  -0.133333   \n",
      "4  repeat order time drive thru still manage mess...   1 star  -0.041071   \n",
      "\n",
      "  sentiment_category  \n",
      "0           Positive  \n",
      "1           Positive  \n",
      "2           Negative  \n",
      "3           Negative  \n",
      "4           Negative  \n"
     ]
    }
   ],
   "source": [
    "#To find positive and negative sentiment using the TextBlob library\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "\n",
    "# LET'S DOWNLOAD NLTK RESOURCES\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#DATAPATH\n",
    "data_path = r\"C:\\Users\\nh013\\Desktop\\MC''donald\\McDonald_s_Reviews.csv\"\n",
    "df = pd.read_csv(data_path, encoding='latin1')\n",
    "\n",
    "# FUNCTION FOR PREPROCESS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "def get_wordnet_pos(token):\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "#PREPROESSING TO THE REVIEW COLUMN\n",
    "df['review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "# SNTIMENT ANALYSIS USING TEXT-BLOB\n",
    "def calculate_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    return sentiment\n",
    "\n",
    "df['sentiment'] = df['review'].apply(calculate_sentiment)\n",
    "\n",
    "# CLASSIFY SENTIMENT AS POSSITIVE OR NEGATIVE\n",
    "df['sentiment_category'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative')\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6806b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667b1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reviewer_id  store_name              category  \\\n",
      "0            1  McDonald's  Fast food restaurant   \n",
      "1            2  McDonald's  Fast food restaurant   \n",
      "2            3  McDonald's  Fast food restaurant   \n",
      "3            4  McDonald's  Fast food restaurant   \n",
      "4            5  McDonald's  Fast food restaurant   \n",
      "\n",
      "                                       store_address  latitude   longitude  \\\n",
      "0  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "1  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "2  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "3  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "4  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "\n",
      "  rating_count   review_time  \\\n",
      "0        1,240  3 months ago   \n",
      "1        1,240    5 days ago   \n",
      "2        1,240    5 days ago   \n",
      "3        1,240   a month ago   \n",
      "4        1,240  2 months ago   \n",
      "\n",
      "                                              review   rating  sentiment  \\\n",
      "0  look like someone spit food normal transaction...   1 star     0.5541   \n",
      "1  mcdonalds far food atmosphere go staff make di...  4 stars     0.8402   \n",
      "2  make mobile order get speaker checked line mov...   1 star    -0.2960   \n",
      "3  mc crispy chicken sandwich customer service qu...  5 stars     0.0000   \n",
      "4  repeat order time drive thru still manage mess...   1 star    -0.7184   \n",
      "\n",
      "  sentiment_category  \n",
      "0           Positive  \n",
      "1           Positive  \n",
      "2           Negative  \n",
      "3           Negative  \n",
      "4           Negative  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Sentiment analysis using vader.\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#  LET'S DOWNLOAD NLTK RESOURCES\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "\n",
    "# DATASET\n",
    "data_path = r\"C:\\Users\\nh013\\Desktop\\MC''donald\\McDonald_s_Reviews.csv\"\n",
    "df = pd.read_csv(data_path, encoding='latin1')\n",
    "\n",
    "\n",
    "# FUNCTION FOR PREPROCESSING\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_pos(token):\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "#PREPROCESSING TO THE REVIEW COLUMN\n",
    "df['review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "#SENTIMENT ANALYSIS USING VADER\n",
    "def calculate_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = sid.polarity_scores(text)['compound']\n",
    "    return sentiment\n",
    "\n",
    "df['sentiment'] = df['review'].apply(calculate_sentiment)\n",
    "\n",
    "# CLASSIFY SENTIMENT AS POSSITVE OR NEGATIVE\n",
    "df['sentiment_category'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative')\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec95b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359513fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ccd915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "835/835 [==============================] - 146s 170ms/step - loss: 1.2169 - val_loss: 0.8996\n",
      "Epoch 2/10\n",
      "835/835 [==============================] - 133s 160ms/step - loss: 0.7221 - val_loss: 0.8241\n",
      "Epoch 3/10\n",
      "835/835 [==============================] - 110s 132ms/step - loss: 0.5947 - val_loss: 0.8366\n",
      "Epoch 4/10\n",
      "835/835 [==============================] - 109s 131ms/step - loss: 0.5224 - val_loss: 0.8264\n",
      "Epoch 5/10\n",
      "835/835 [==============================] - 113s 135ms/step - loss: 0.4671 - val_loss: 0.8141\n",
      "Epoch 6/10\n",
      "835/835 [==============================] - 109s 130ms/step - loss: 0.4175 - val_loss: 0.8220\n",
      "Epoch 7/10\n",
      "835/835 [==============================] - 111s 133ms/step - loss: 0.3845 - val_loss: 0.8425\n",
      "Epoch 8/10\n",
      "835/835 [==============================] - 114s 136ms/step - loss: 0.3542 - val_loss: 0.8204\n",
      "Epoch 9/10\n",
      "835/835 [==============================] - 117s 141ms/step - loss: 0.3321 - val_loss: 0.8277\n",
      "Epoch 10/10\n",
      "835/835 [==============================] - 124s 149ms/step - loss: 0.3088 - val_loss: 0.8495\n",
      "209/209 [==============================] - 10s 40ms/step\n",
      "Top 10 High-Performing Regions:\n",
      "                                        store_address  predicted_rating\n",
      "28  5920 Balboa Ave, San Diego, CA 92111, United S...          3.829932\n",
      "34  702-2 Haddonfield-Berlin Rd, Voorhees Township...          3.641321\n",
      "26  555 13th St NW, Washington, DC 20004, United S...          3.638682\n",
      "31  6875 Sand Lake Rd, Orlando, FL 32819, United S...          3.628165\n",
      "22     429 7th Ave, New York, NY 10001, United States          3.597639\n",
      "14  1698 US-209, Brodheadsville, PA 18322, United ...          3.512563\n",
      "3   1100 N US Hwy 377, Roanoke, TX 76262, United S...          3.510272\n",
      "4   111 Madison St, Oak Park, IL 60302, United States          3.468577\n",
      "33  7010 Bradlick Shopping Center, Annandale, VA 2...          3.445307\n",
      "9   1415 E State Rd, Fern Park, FL 32730, United S...          3.397718\n",
      "\n",
      "Bottom 10 Underperforming Regions:\n",
      "                                        store_address  predicted_rating\n",
      "24  501 W Imperial Hwy, Los Angeles, CA 90044, Uni...          2.678558\n",
      "27  5725 W Irlo Bronson Memorial Hwy, Kissimmee, F...          2.641564\n",
      "38  9814 International Dr, Orlando, FL 32819, Unit...          2.614086\n",
      "21  3501 Biscayne Blvd, Miami, FL 33137, United St...          2.554254\n",
      "16  210 5th S, Salt Lake City, UT 84106, United St...          2.540840\n",
      "10  151 West 34th Street (Macy's 7th Floor, New Yo...          2.470300\n",
      "13  1650 Washington Ave, Miami Beach, FL 33139, Un...          1.932187\n",
      "1   10451 Santa Monica Blvd, Los Angeles, CA 90025...               NaN\n",
      "7   114 Delancey St, New York, NY 10002, United St...               NaN\n",
      "12    160 Broadway, New York, NY 10038, United States               NaN\n"
     ]
    }
   ],
   "source": [
    "# TRAIN  RNN'S MODEL TO Explore geographical patterns in reviews and ratings to identify high-performing or \n",
    "#underperforming regions.\n",
    "\n",
    "\n",
    "#Location-based analysis\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "# LET'S DOWNLOAD NLTK RESOURCES\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "# DATASET\n",
    "data_path = r\"C:\\Users\\nh013\\Desktop\\MC''donald\\McDonald_s_Reviews.csv\"\n",
    "df = pd.read_csv(data_path, encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "# FUNCTION FOR PREPROCESSING\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "def get_wordnet_pos(token):\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESS TO THE REVIEW COLUMN\n",
    "df['review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# CLEAN THE RATING COLUMN\n",
    "df['rating'] = df['rating'].apply(lambda x: re.sub(r'[^0-9.]', '', x))\n",
    "df['rating'] = df['rating'].astype(float)\n",
    "\n",
    "\n",
    "# SPLIT DATA INTO TRAINING AND TESTING SET\n",
    "X = df['review']\n",
    "y = df['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# TOKENIZE THE TEXT DATA AND CONVERT IT TO SEQUENCE\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# PAD THE SEQUENCE TO FIX LENGTH\n",
    "max_sequence_length = 100\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# TRAIN MODEL\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "# COMPILE AND TRAIN THE MODEL\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "# PREDICT THE RATING USING THE TRAIN MOEL\n",
    "predictions = model.predict(X_test_padded)\n",
    "\n",
    "\n",
    "# FLATTEN THE PREDICTION ARRAY\n",
    "predictions = predictions.flatten()\n",
    "\n",
    "\n",
    "#ADD THE PREDICTION TO THE DATAFRAME\n",
    "df.loc[X_test.index, 'predicted_rating'] = predictions\n",
    "\n",
    "\n",
    "\n",
    "# EXPLORE GEOGRAPHICAL-PATTERN  BASED ON THE PREDICTED RATINGS\n",
    "location_df = df[['store_address', 'latitude', 'longitude', 'predicted_rating']].copy()\n",
    "\n",
    "# GROUP THE DATAFAME BY STORE ADDRESS AND CALCULATE AVERAGE PREDICTED RATINGS\n",
    "average_predicted_rating_by_location = location_df.groupby('store_address')['predicted_rating'].mean().reset_index()\n",
    "\n",
    "\n",
    "#SORT THE LOCATION BY AVERAGE PREDICTING RATING IN DESENDING ORDER\n",
    "sorted_locations = average_predicted_rating_by_location.sort_values('predicted_rating', ascending=False)\n",
    "\n",
    "#PRINT THE TOP 10 HIGH PERFORMGING REGION\n",
    "print(\"Top 10 High-Performing Regions:\")\n",
    "print(sorted_locations.head(10))\n",
    "\n",
    "# PRINT THE BOTTOM 10 UNDERPERFORMIMG REGION\n",
    "print(\"\\nBottom 10 Underperforming Regions:\")\n",
    "print(sorted_locations.tail(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
